# Topic: Reinforcement Learning II (Continued)



## Sub_Topics neccessary for the current chapter are

<details>
  <summary>MDP -> Markov Decision Process:</summary>

# ğŸ§  CS5700: Artificial Intelligence  
## ğŸ“˜ Chapter 10 Deep Dive â€“ Markov Decision Process (MDP)

---

## ğŸŒŸ Overview

A **Markov Decision Process (MDP)** is a mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of an agent.

> Itâ€™s the foundation of everything in **Reinforcement Learning**.

---

## âœ… Learning Tracker

| Concept | Status |
|--------|--------|
| Understand states, actions, rewards | â˜‘ï¸ |
| Explain the Markov property | â˜‘ï¸ |
| Differentiate policy and transition function | â˜‘ï¸ |
| Understand stochasticity in real-world actions | â˜‘ï¸ |

---

## ğŸ§± Components of an MDP

An MDP is defined as a 5-tuple:

```math
MDP = (S, A, T, R, Î³)
```

| Symbol | Description |
|--------|-------------|
| `S` | Set of states |
| `A` | Set of actions |
| `T(s, a, s')` | Transition probability: P(next state `s'` | current state `s`, action `a`) |
| `R(s, a, s')` | Reward for taking action `a` in state `s` and ending up in `s'` |
| `Î³` | Discount factor (importance of future rewards) |

---

## ğŸ§  Jungle Survival: Real-World Analogy

```
You are lost in a jungle...

         [Muddy Hill]
               |
      (follow footprints)
               â†“
           [River] â† â† â† [Snake Pit]
               |
         (climb tree)
               â†“
        [Fruit Grove] â†’ Reward +10
```

| Concept | Jungle Example |
|--------|----------------|
| `State (S)` | Your current location (e.g., Muddy Hill) |
| `Action (A)` | Walk, climb, follow tracks |
| `T(s,a,s')` | Following tracks from Muddy Hill â†’ 70% River, 30% Snake Pit |
| `R(s,a,s')` | River = +5, Snake Pit = -10 |
| `Ï€(s)` | If you hear birds â†’ follow tracks |

---

## ğŸ“œ The Markov Property

> â€œThe future depends only on the present, not the full history.â€

```
Past:     sâ‚€ â€” aâ‚€ â€” sâ‚ â€” aâ‚ â€” sâ‚‚
Now:                            â†‘

Markov Assumption:
P(sâ‚ƒ | sâ‚‚, aâ‚‚) = P(sâ‚ƒ | sâ‚€, aâ‚€, sâ‚, aâ‚, sâ‚‚, aâ‚‚)
```

This simplifies learning and planning â€” only **current state** matters.

âœ… Efficient
âœ… Realistic in many domains
âœ… Enables recursion like Bellman updates

---

## ğŸ¤– Why MDPs Are Made for Reinforcement Learning

- Models **interaction**: Take action â†’ observe result â†’ learn from reward
- Supports **delayed rewards**: Actions today may pay off later
- Allows **exploration**: Try things to learn about consequences

---

## ğŸ“˜ Policy vs Transition Function

| Concept | What it Does | Analogy |
|--------|---------------|---------|
| `Ï€(s)` | Tells what **action** to take in each state | Your internal rulebook: "At yellow light â†’ slow down" |
| `T(s,a,s')` | Tells where you **might land** if you take an action in a state | Physics: "Braking on a wet road has a 30% chance of skidding" |

---

## ğŸ² Why Actions Are Stochastic

In the real world:
- Robots slip
- Environments are noisy
- Humans make imprecise moves

> The same action from the same state can have **multiple possible outcomes** â€” hence, we model transitions as probabilities.

```text
          [State S]
             |
         (Action A)
             â†“
     --------------------
    |                    |
 70%                  30%
[State S']         [State S'']
```

---

## ğŸ’¼ Real-World Applications

| Domain         | MDP Role |
|----------------|----------|
| ğŸ® Game AI      | Strategy adaptation, enemy modeling |
| ğŸš— Self-Driving | Navigation, obstacle avoidance |
| ğŸ¤– Robotics     | Action planning under uncertainty |
| ğŸ’¹ Finance      | Trade decision-making over time |
| ğŸ¥ Healthcare   | Patient treatment planning |

---

## ğŸ§ª Quiz Yourself

- [ ] Can I define all 5 components of an MDP?
- [ ] Can I give a real-world example of a transition function?
- [ ] Can I explain why we use probabilities for outcomes?
- [ ] Can I describe the Markov property in simple words and in math?

---

## ğŸ“Œ Summary

MDPs model:
- **Where** the agent is (`S`)
- **What** it can do (`A`)
- **What happens** when it does (`T`)
- **How good** the outcome is (`R`)
- **How much the future matters** (`Î³`)

âœ… They are **essential** for any reinforcement learning system.



> **In reinforcement learning, since the transition dynamics (`T`) and reward structure (`R`) are unknown, the agent must learn them through interaction with the environment.**
  
</details>

<details>

<summary>Value Functions & Bellman Equations  </summary>

# ğŸ”¢ Value Functions & Bellman Equations  
## ğŸ“˜ Chapter 10 â€“ Topic 2: Estimating How Good States and Actions Are

---

## ğŸ§  What Is a Value Function?

A **value function** tells you how good it is to be in a state (or take an action), assuming you follow a particular policy `Ï€`.

> It answers the question:  
> **â€œIf I start in this state, how much reward am I likely to get over time?â€**

---

## ğŸ“Š Two Key Types of Value Functions

| Function | Description |
|---------|-------------|
| `VÏ€(s)` | Expected cumulative reward starting from state `s` and following policy `Ï€` |
| `QÏ€(s, a)` | Expected cumulative reward from state `s`, taking action `a`, then following policy `Ï€` |

---

## ğŸ§­ Real-Life Analogy: Hiking with a Map

Imagine you're in a foggy forest trying to reach a treasure:

- `VÏ€(s)` tells you the **value of being where you are now**, assuming you follow your current path.
- `QÏ€(s, a)` tells you the **value of taking a specific next step**, then continuing on the known path.

---

## â³ Discount Factor `Î³`

- `Î³ âˆˆ [0, 1]` controls how much future rewards matter:
  - `Î³ â‰ˆ 0` â†’ you care only about immediate rewards (short-sighted)
  - `Î³ â‰ˆ 1` â†’ you care about long-term gains (patient)

---

## ğŸ“ Bellman Expectation Equation (for `VÏ€(s)`)

```math
VÏ€(s) = Î£â‚ Ï€(s, a) Î£_{s'} T(s, a, s') [ R(s, a, s') + Î³ VÏ€(s') ]
```

> Recursive definition:  
> **"Current state's value = expected immediate reward + discounted future values"**

---

## ğŸ§± Intuition: Recursive LEGO

Each stateâ€™s value is **built from** the values of the states it can reach â€” connected by the **action probabilities** and the **rewards received**.

---

## ğŸ“ Bellman Equation for Q-Values

```math
QÏ€(s, a) = Î£_{s'} T(s, a, s') [ R(s, a, s') + Î³ Î£_{a'} Ï€(s', a') QÏ€(s', a') ]
```

This computes the value of **taking action `a` in state `s`**, then following the policy.

---

## ğŸ§® Visualization: Decision Flow

```
   [State s]
      |
   Ï€(s, a)
      |
  Take action a
      â†“
  Transitions to s'
      |
  Get R(s,a,s') + Î³ V(s')
```

The value is the **average of all these possible outcomes**.

---

## âœ… Why This Matters

- Central to **policy evaluation**
- Powers **Value Iteration** and **Policy Iteration**
- Forms the mathematical core of **Q-learning** and **Deep RL**

---

## ğŸ§  Real-World Applications

| Domain      | Use of Value Functions |
|-------------|------------------------|
| ğŸ§  Game AI    | Decide the best move based on expected win/loss |
| ğŸš— Self-driving | Estimate safety of each maneuver |
| ğŸ“ˆ Finance    | Evaluate risk-adjusted future returns |
| ğŸ¤– Robotics   | Optimize action sequences over time |
| ğŸ¥ Healthcare | Estimate expected outcomes of treatment paths |

---

## âœ… Knowledge Checklist

- [x] Know the difference between `VÏ€(s)` and `QÏ€(s, a)`
- [x] Understand the role of the discount factor `Î³`
- [x] Can explain the Bellman Equation with examples
- [x] Know how value functions guide better decisions in RL


</details>

<details>
  <summary> ğŸ“˜ Reinforcement Learning â€“ Formula Sheet (MDPs, Bellman, Q-Learning)  </summary>

> All essential formulas from Chapter 10 and connected topics  
> With meanings, use-cases, and memory tricks for mastery

---

## ğŸ“ Core MDP Structure

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `MDP = (S, A, T, R, Î³)` | Set of States, Actions, Transitions, Rewards, Discount | Defines RL problem | **"SATR Gamma" is your world** |

---

## ğŸ§  Value Functions

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `VÏ€(s) = E[ râ‚€ + Î³râ‚ + Î³Â²râ‚‚ + â€¦ ]` | Value of state `s` under policy `Ï€` | Evaluate states | **"How good is being here?"** |
| `QÏ€(s, a) = E[ râ‚€ + Î³râ‚ + Î³Â²râ‚‚ + â€¦ ]` | Value of action `a` in state `s` under `Ï€` | Evaluate actions | **"Quality of an action"** |

---

## ğŸ§¾ Bellman Equations (Expectation)

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `VÏ€(s) = Î£â‚ Ï€(s, a) Î£_{s'} T(s, a, s') [ R(s, a, s') + Î³ VÏ€(s') ]` | Recursive value definition for a state | Policy evaluation | **"Current value = avg reward + future"** |
| `QÏ€(s, a) = Î£_{s'} T(s, a, s') [ R(s, a, s') + Î³ Î£_{a'} Ï€(s', a') QÏ€(s', a') ]` | Recursive action-value | Policy evaluation | **"Q from future Qs"** |

---

## ğŸ Bellman Optimality Equations

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `V*(s) = maxâ‚ Î£_{s'} T(s, a, s') [ R(s, a, s') + Î³ V*(s') ]` | Best possible value from `s` | Planning | **"Choose best action"** |
| `Q*(s, a) = Î£_{s'} T(s, a, s') [ R(s, a, s') + Î³ maxâ‚' Q*(s', a') ]` | Best value of `(s, a)` | Planning | **"Q of now = best Q of future"** |

---

## ğŸ” Q-Learning (Model-Free)

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `Q(s, a) â† Q(s, a) + Î± [ r + Î³ maxâ‚' Q(s', a') âˆ’ Q(s, a) ]` | Learn action values by trial and error | Model-free learning | **"Q = Q + trust Ã— surprise"** |

---

## ğŸ² Exploration: Îµ-Greedy Policy

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `Ï€(s) =`<br> `â§ random action with prob Îµ`<br> `â© argmaxâ‚ Q(s, a) with prob 1 - Îµ` | Exploration strategy | Balance trying vs. exploiting | **"Flip coin: random vs best"** |

---

## ğŸ§  Final Notes

- `Î±`: Learning rate â†’ How quickly to trust new info (0 = never, 1 = overwrite)
- `Î³`: Discount factor â†’ Importance of future (0 = only now, 1 = long-term)
- `r`: Immediate reward from action
- `s, a, s'`: State, action, next state
- `Q(s, a)`: Expected total reward starting from `s`, doing `a`


</details>
