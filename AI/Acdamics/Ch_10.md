# Topic: Reinforcement Learning II (Continued)



## Sub_Topics neccessary for the current chapter are

<details>
  <summary>MDP -> Markov Decision Process:</summary>

# 🧠 CS5700: Artificial Intelligence  
## 📘 Chapter 10 Deep Dive – Markov Decision Process (MDP)

---

## 🌟 Overview

A **Markov Decision Process (MDP)** is a mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of an agent.

> It’s the foundation of everything in **Reinforcement Learning**.

---

## ✅ Learning Tracker

| Concept | Status |
|--------|--------|
| Understand states, actions, rewards | ☑️ |
| Explain the Markov property | ☑️ |
| Differentiate policy and transition function | ☑️ |
| Understand stochasticity in real-world actions | ☑️ |

---

## 🧱 Components of an MDP

An MDP is defined as a 5-tuple:

```math
MDP = (S, A, T, R, γ)
```

| Symbol | Description |
|--------|-------------|
| `S` | Set of states |
| `A` | Set of actions |
| `T(s, a, s')` | Transition probability: P(next state `s'` | current state `s`, action `a`) |
| `R(s, a, s')` | Reward for taking action `a` in state `s` and ending up in `s'` |
| `γ` | Discount factor (importance of future rewards) |

---

## 🧠 Jungle Survival: Real-World Analogy

```
You are lost in a jungle...

         [Muddy Hill]
               |
      (follow footprints)
               ↓
           [River] ← ← ← [Snake Pit]
               |
         (climb tree)
               ↓
        [Fruit Grove] → Reward +10
```

| Concept | Jungle Example |
|--------|----------------|
| `State (S)` | Your current location (e.g., Muddy Hill) |
| `Action (A)` | Walk, climb, follow tracks |
| `T(s,a,s')` | Following tracks from Muddy Hill → 70% River, 30% Snake Pit |
| `R(s,a,s')` | River = +5, Snake Pit = -10 |
| `π(s)` | If you hear birds → follow tracks |

---

## 📜 The Markov Property

> “The future depends only on the present, not the full history.”

```
Past:     s₀ — a₀ — s₁ — a₁ — s₂
Now:                            ↑

Markov Assumption:
P(s₃ | s₂, a₂) = P(s₃ | s₀, a₀, s₁, a₁, s₂, a₂)
```

This simplifies learning and planning — only **current state** matters.

✅ Efficient
✅ Realistic in many domains
✅ Enables recursion like Bellman updates

---

## 🤖 Why MDPs Are Made for Reinforcement Learning

- Models **interaction**: Take action → observe result → learn from reward
- Supports **delayed rewards**: Actions today may pay off later
- Allows **exploration**: Try things to learn about consequences

---

## 📘 Policy vs Transition Function

| Concept | What it Does | Analogy |
|--------|---------------|---------|
| `π(s)` | Tells what **action** to take in each state | Your internal rulebook: "At yellow light → slow down" |
| `T(s,a,s')` | Tells where you **might land** if you take an action in a state | Physics: "Braking on a wet road has a 30% chance of skidding" |

---

## 🎲 Why Actions Are Stochastic

In the real world:
- Robots slip
- Environments are noisy
- Humans make imprecise moves

> The same action from the same state can have **multiple possible outcomes** — hence, we model transitions as probabilities.

```text
          [State S]
             |
         (Action A)
             ↓
     --------------------
    |                    |
 70%                  30%
[State S']         [State S'']
```

---

## 💼 Real-World Applications

| Domain         | MDP Role |
|----------------|----------|
| 🎮 Game AI      | Strategy adaptation, enemy modeling |
| 🚗 Self-Driving | Navigation, obstacle avoidance |
| 🤖 Robotics     | Action planning under uncertainty |
| 💹 Finance      | Trade decision-making over time |
| 🏥 Healthcare   | Patient treatment planning |

---

## 🧪 Quiz Yourself

- [ ] Can I define all 5 components of an MDP?
- [ ] Can I give a real-world example of a transition function?
- [ ] Can I explain why we use probabilities for outcomes?
- [ ] Can I describe the Markov property in simple words and in math?

---

## 📌 Summary

MDPs model:
- **Where** the agent is (`S`)
- **What** it can do (`A`)
- **What happens** when it does (`T`)
- **How good** the outcome is (`R`)
- **How much the future matters** (`γ`)

✅ They are **essential** for any reinforcement learning system.



> **In reinforcement learning, since the transition dynamics (`T`) and reward structure (`R`) are unknown, the agent must learn them through interaction with the environment.**
  
</details>

<details>

<summary>Value Functions & Bellman Equations  </summary>

# 🔢 Value Functions & Bellman Equations  
## 📘 Chapter 10 – Topic 2: Estimating How Good States and Actions Are

---

## 🧠 What Is a Value Function?

A **value function** tells you how good it is to be in a state (or take an action), assuming you follow a particular policy `π`.

> It answers the question:  
> **“If I start in this state, how much reward am I likely to get over time?”**

---

## 📊 Two Key Types of Value Functions

| Function | Description |
|---------|-------------|
| `Vπ(s)` | Expected cumulative reward starting from state `s` and following policy `π` |
| `Qπ(s, a)` | Expected cumulative reward from state `s`, taking action `a`, then following policy `π` |

---

## 🧭 Real-Life Analogy: Hiking with a Map

Imagine you're in a foggy forest trying to reach a treasure:

- `Vπ(s)` tells you the **value of being where you are now**, assuming you follow your current path.
- `Qπ(s, a)` tells you the **value of taking a specific next step**, then continuing on the known path.

---

## ⏳ Discount Factor `γ`

- `γ ∈ [0, 1]` controls how much future rewards matter:
  - `γ ≈ 0` → you care only about immediate rewards (short-sighted)
  - `γ ≈ 1` → you care about long-term gains (patient)

---

## 📐 Bellman Expectation Equation (for `Vπ(s)`)

```math
Vπ(s) = Σₐ π(s, a) Σ_{s'} T(s, a, s') [ R(s, a, s') + γ Vπ(s') ]
```

> Recursive definition:  
> **"Current state's value = expected immediate reward + discounted future values"**

---

## 🧱 Intuition: Recursive LEGO

Each state’s value is **built from** the values of the states it can reach — connected by the **action probabilities** and the **rewards received**.

---

## 📐 Bellman Equation for Q-Values

```math
Qπ(s, a) = Σ_{s'} T(s, a, s') [ R(s, a, s') + γ Σ_{a'} π(s', a') Qπ(s', a') ]
```

This computes the value of **taking action `a` in state `s`**, then following the policy.

---

## 🧮 Visualization: Decision Flow

```
   [State s]
      |
   π(s, a)
      |
  Take action a
      ↓
  Transitions to s'
      |
  Get R(s,a,s') + γ V(s')
```

The value is the **average of all these possible outcomes**.

---

## ✅ Why This Matters

- Central to **policy evaluation**
- Powers **Value Iteration** and **Policy Iteration**
- Forms the mathematical core of **Q-learning** and **Deep RL**

---

## 🧠 Real-World Applications

| Domain      | Use of Value Functions |
|-------------|------------------------|
| 🧠 Game AI    | Decide the best move based on expected win/loss |
| 🚗 Self-driving | Estimate safety of each maneuver |
| 📈 Finance    | Evaluate risk-adjusted future returns |
| 🤖 Robotics   | Optimize action sequences over time |
| 🏥 Healthcare | Estimate expected outcomes of treatment paths |

---

## ✅ Knowledge Checklist

- [x] Know the difference between `Vπ(s)` and `Qπ(s, a)`
- [x] Understand the role of the discount factor `γ`
- [x] Can explain the Bellman Equation with examples
- [x] Know how value functions guide better decisions in RL


</details>

<details>
  <summary> 📘 Reinforcement Learning – Formula Sheet (MDPs, Bellman, Q-Learning)  </summary>

> All essential formulas from Chapter 10 and connected topics  
> With meanings, use-cases, and memory tricks for mastery

---

## 📐 Core MDP Structure

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `MDP = (S, A, T, R, γ)` | Set of States, Actions, Transitions, Rewards, Discount | Defines RL problem | **"SATR Gamma" is your world** |

---

## 🧠 Value Functions

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `Vπ(s) = E[ r₀ + γr₁ + γ²r₂ + … ]` | Value of state `s` under policy `π` | Evaluate states | **"How good is being here?"** |
| `Qπ(s, a) = E[ r₀ + γr₁ + γ²r₂ + … ]` | Value of action `a` in state `s` under `π` | Evaluate actions | **"Quality of an action"** |

---

## 🧾 Bellman Equations (Expectation)

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `Vπ(s) = Σₐ π(s, a) Σ_{s'} T(s, a, s') [ R(s, a, s') + γ Vπ(s') ]` | Recursive value definition for a state | Policy evaluation | **"Current value = avg reward + future"** |
| `Qπ(s, a) = Σ_{s'} T(s, a, s') [ R(s, a, s') + γ Σ_{a'} π(s', a') Qπ(s', a') ]` | Recursive action-value | Policy evaluation | **"Q from future Qs"** |

---

## 🏁 Bellman Optimality Equations

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `V*(s) = maxₐ Σ_{s'} T(s, a, s') [ R(s, a, s') + γ V*(s') ]` | Best possible value from `s` | Planning | **"Choose best action"** |
| `Q*(s, a) = Σ_{s'} T(s, a, s') [ R(s, a, s') + γ maxₐ' Q*(s', a') ]` | Best value of `(s, a)` | Planning | **"Q of now = best Q of future"** |

---

## 🔁 Q-Learning (Model-Free)

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `Q(s, a) ← Q(s, a) + α [ r + γ maxₐ' Q(s', a') − Q(s, a) ]` | Learn action values by trial and error | Model-free learning | **"Q = Q + trust × surprise"** |

---

## 🎲 Exploration: ε-Greedy Policy

| Formula | Description | Use | Memory Trick |
|--------|-------------|-----|--------------|
| `π(s) =`<br> `⎧ random action with prob ε`<br> `⎩ argmaxₐ Q(s, a) with prob 1 - ε` | Exploration strategy | Balance trying vs. exploiting | **"Flip coin: random vs best"** |

---

## 🧠 Final Notes

- `α`: Learning rate → How quickly to trust new info (0 = never, 1 = overwrite)
- `γ`: Discount factor → Importance of future (0 = only now, 1 = long-term)
- `r`: Immediate reward from action
- `s, a, s'`: State, action, next state
- `Q(s, a)`: Expected total reward starting from `s`, doing `a`


</details>
